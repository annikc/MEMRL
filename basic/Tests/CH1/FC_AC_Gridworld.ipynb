{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "import pickle\n",
    "sys.path.insert(0, './../../modules')\n",
    "\n",
    "from Utils import running_mean as rm\n",
    "from Utils import one_hot_state, onehot_state_collection, twoD_states\n",
    "\n",
    "# import representation type\n",
    "from Agents.RepresentationLearning import PlaceCells\n",
    "# import actor critic network\n",
    "from Agents.Networks import ActorCritic as Network\n",
    "# import agent class wrapper to handle behaviour\n",
    "from Agents import Agent\n",
    "# import experiment class to handle run and logging\n",
    "from Experiments import expt\n",
    "\n",
    "\n",
    "# get environment\n",
    "import gym\n",
    "sys.path.append('../../../basic/')\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from Agents.EpisodicMemory import EpisodicMemory as Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 226.8x252 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make env\n",
    "env_name = 'gym_grid:gridworld-v6'\n",
    "env = gym.make(env_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_state_reps = {}\n",
    "    \n",
    "place_cells = PlaceCells(env.shape, env.nstates, field_size=0.1)\n",
    "pc_state_reps = {}\n",
    "\n",
    "for state in env.useable:\n",
    "    oh_state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "    pc_state_reps[env.twoD2oneD(state)] = place_cells.get_activities([state])[0]\n",
    "\n",
    "input_dims = len(oh_state_reps[list(oh_state_reps.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(oh_state_reps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [0.33440732955932617s]\n",
      "Episode: 100, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.38099670410156s]\n",
      "Episode: 200, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.469529151916504s]\n",
      "Episode: 300, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.73510718345642s]\n",
      "Episode: 400, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.93386149406433s]\n",
      "Episode: 500, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.13181138038635s]\n",
      "Episode: 600, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.892024993896484s]\n",
      "Episode: 700, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [40.002134799957275s]\n",
      "Episode: 800, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.922871589660645s]\n",
      "Episode: 900, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [40.36606431007385s]\n",
      "Episode: 1000, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.1575448513031s]\n",
      "Episode: 1100, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.19744300842285s]\n",
      "Episode: 1200, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.78023982048035s]\n",
      "Episode: 1300, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.52947807312012s]\n",
      "Episode: 1400, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.304566383361816s]\n",
      "Episode: 1500, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [38.850545167922974s]\n",
      "Episode: 1600, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.85782742500305s]\n",
      "Episode: 1700, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [39.79002237319946s]\n",
      "Episode: 1800, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [90.96371746063232s]\n"
     ]
    }
   ],
   "source": [
    "## onehot state representations\n",
    "for _ in range(1):\n",
    "    oh_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    oh_agent = Agent(oh_network, state_representations=oh_state_reps)\n",
    "\n",
    "    ex = expt(oh_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'onehot_train'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## place cell representations\n",
    "for _ in range(5):\n",
    "    pc_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    pc_agent = Agent(pc_network, state_representations=pc_state_reps)\n",
    "\n",
    "    ex = expt(pc_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'place_cell_train'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "    \n",
    "    ex.data['place_cells'] = place_cells\n",
    "    \n",
    "    extras = [place_cells.field_size]\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename, extra=extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place cell representations\n",
    "for f in [0.1]:\n",
    "    for e in range(5):\n",
    "        env_name = f'gym_grid:gridworld-v{e+1}'\n",
    "        env = gym.make(env_name)\n",
    "        place_cells = PlaceCells(env.shape, 200, field_size=f)\n",
    "        pc_state_reps = {}\n",
    "\n",
    "        for state in env.useable:\n",
    "            pc_state_reps[env.twoD2oneD(state)] = place_cells.get_activities([state])[0]\n",
    "            \n",
    "        input_dims = len(pc_state_reps[list(pc_state_reps.keys())[0]])\n",
    "\n",
    "        for _ in range(5):\n",
    "            pc_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "            pc_agent = Agent(pc_network, state_representations=pc_state_reps)\n",
    "\n",
    "            ex = expt(pc_agent,env)\n",
    "\n",
    "            num_trials = 5000\n",
    "            num_events = 250\n",
    "            ex.run(num_trials, num_events)\n",
    "\n",
    "            expt_type = 'place_cell_train'\n",
    "            directory = '../../Data/'\n",
    "            filename  = 'mf_training.csv'\n",
    "\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "            extras = [input_dims, place_cells.field_size]\n",
    "\n",
    "            ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename, extra=extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# successor representations\n",
    "with open(f'../../modules/Agents/RepresentationLearning/SR_{env_name}.p', 'rb') as f:\n",
    "    sr_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_reps = {}\n",
    "SR = np.sum(sr_, axis = 0)\n",
    "for index in range(SR.shape[0]):\n",
    "    sr_reps[index] = SR[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 250\n",
    "plt.figure()\n",
    "plt.imshow(sr_reps[index].reshape(*env.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## successor representations\n",
    "sr_reps = {}\n",
    "SR = np.sum(sr_, axis = 0)\n",
    "for index in range(SR.shape[0]):\n",
    "    sr_reps[index] = SR[index]\n",
    "    \n",
    "for _ in range(1):\n",
    "    input_dims = len(sr_reps[list(sr_reps.keys())[0]])\n",
    "    \n",
    "    sr_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    sr_agent = Agent(sr_network, state_representations=sr_reps)\n",
    "\n",
    "    ex = expt(sr_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'sr_train_with_rec'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex.data['P_snap'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.gridworld_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5000-1\n",
    "plot_pref_pol(env, ex.data['P_snap'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing \n",
    "df = pd.read_csv('../../Data/mf_training.csv')\n",
    "\n",
    "for j in range(len(df)):\n",
    "    print(i)\n",
    "    i = int(j+130)\n",
    "    line = df.iloc[i]\n",
    "    if line['pass']=='pass':\n",
    "        pass\n",
    "    else:\n",
    "        run_id    = line['run_id']\n",
    "        env_type  = line['env_type']\n",
    "        expt_type = line['expt_type']\n",
    "        num_cells = line['Num_inputs']\n",
    "\n",
    "        # make environment\n",
    "        env = gym.make(env_type)\n",
    "        # remap reward \n",
    "        env.set_reward({(15,15):10})\n",
    "\n",
    "        # make network\n",
    "        network = torch.load(f'../../Data/agents/{run_id}.pt')\n",
    "\n",
    "        # get state representations\n",
    "        if expt_type == 'onehot_train':\n",
    "            print(\"Using Onehot Representation\")\n",
    "            state_reps = {}\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "\n",
    "        elif expt_type == 'place_cell_train':\n",
    "            print(\"Using Place Cell Representation\")\n",
    "            fs = line['PlaceCell_fieldsize']\n",
    "            with open(f'../../Data/results/{run_id}_data.p', 'rb') as f:\n",
    "                cell_centres = pickle.load(f)['place_cells']\n",
    "            state_reps = {}\n",
    "            pcs = PlaceCells(env.shape,num_cells=int(num_cells), field_size=fs, cell_centres=cell_centres)\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = pcs.get_activities([state])[0]\n",
    "\n",
    "        if expt_type[0:8] == 'sr_train':\n",
    "            print(\"Using Successor Representation\")\n",
    "            with open(f'../../modules/Agents/RepresentationLearning/SR_{env_type}.p', 'rb') as f:\n",
    "                sr_ = pickle.load(f)\n",
    "\n",
    "            state_reps = {}\n",
    "            SR = np.sum(sr_, axis = 0)\n",
    "            for index in range(SR.shape[0]):\n",
    "                state_reps[index] = SR[index]\n",
    "\n",
    "        # make agent\n",
    "        agent = Agent(network, state_representations=state_reps)\n",
    "\n",
    "        # make experiment\n",
    "        ex = expt(agent,env)\n",
    "\n",
    "        num_trials = 5000\n",
    "        num_events = 250\n",
    "        # run experiment\n",
    "        ex.run(num_trials, num_events)\n",
    "\n",
    "        # log results \n",
    "        expt_type = f'{expt_type[:-6]}_testing'\n",
    "        directory = '../../Data/'\n",
    "        filename  = 'mf_testing.csv'\n",
    "\n",
    "        if expt_type == 'place_cell_train':\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "        extras = [run_id]\n",
    "\n",
    "        ex.record_log(expt_type, env_type, num_trials, num_events, dir=directory, file=filename, extra=extras,\n",
    "                     mock_log=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expt_type[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_reps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expt_type[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with EC\n",
    "### testing \n",
    "df = pd.read_csv('../../Data/mf_training.csv')\n",
    "\n",
    "for j in range(len(df)):    \n",
    "    i = int(j)\n",
    "    if i ==0:\n",
    "        pass\n",
    "    line = df.iloc[i]\n",
    "    if line['pass']=='pass':\n",
    "        pass\n",
    "    else:\n",
    "        run_id    = line['run_id']\n",
    "        env_type  = line['env_type']\n",
    "        expt_type = line['expt_type']\n",
    "        num_cells = line['Num_inputs']\n",
    "\n",
    "        # make environment\n",
    "        env = gym.make(env_type)\n",
    "        # remap reward \n",
    "        env.set_reward({(15,15):10})\n",
    "\n",
    "        # make network\n",
    "        network = torch.load(f'../../Data/agents/{run_id}.pt')\n",
    "\n",
    "        # get state representations\n",
    "        if expt_type == 'onehot_train':\n",
    "            print(\"Using Onehot Representation\")\n",
    "            state_reps = {}\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "\n",
    "        elif expt_type == 'place_cell_train':\n",
    "            print(\"Using Place Cell Representation\")\n",
    "            fs = line['PlaceCell_fieldsize']\n",
    "            with open(f'../../Data/results/{run_id}_data.p', 'rb') as f:\n",
    "                cell_centres = pickle.load(f)['place_cells']\n",
    "            state_reps = {}\n",
    "            pcs = PlaceCells(env.shape,num_cells=int(num_cells), field_size=fs, cell_centres=cell_centres)\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = pcs.get_activities([state])[0]\n",
    "\n",
    "        if expt_type[0:8] == 'sr_train':\n",
    "            print(\"Using Successor Representation\")\n",
    "            with open(f'../../modules/Agents/RepresentationLearning/SR_{env_type}.p', 'rb') as f:\n",
    "                sr_ = pickle.load(f)\n",
    "\n",
    "            state_reps = {}\n",
    "            SR = np.sum(sr_, axis = 0)\n",
    "            for index in range(SR.shape[0]):\n",
    "                state_reps[index] = SR[index]\n",
    "        \n",
    "        # make memory\n",
    "        memory = Memory(cache_limit=400, entry_size=env.action_space.n)\n",
    "        # make agent\n",
    "        agent = Agent(network, state_representations=state_reps, memory=memory)\n",
    "        agent.get_action = agent.EC_action\n",
    "        # make experiment\n",
    "        ex = expt(agent,env)\n",
    "\n",
    "        num_trials = 1000\n",
    "        num_events = 250\n",
    "        # run experiment\n",
    "        ex.run(num_trials, num_events)\n",
    "\n",
    "        # log results \n",
    "        expt_type = f'{expt_type[:-6]}_testing'\n",
    "        directory = '../../Data/'\n",
    "        filename  = 'ec_testing.csv'\n",
    "\n",
    "        if expt_type == 'place_cell_train':\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "        extras = [run_id]\n",
    "\n",
    "        ex.record_log(expt_type, env_type, num_trials, num_events, dir=directory, file=filename, extra=extras,\n",
    "                     mock_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
