{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "import pickle\n",
    "sys.path.insert(0, './../../modules')\n",
    "\n",
    "from Utils import running_mean as rm\n",
    "from Utils import one_hot_state, onehot_state_collection, twoD_states\n",
    "\n",
    "# import representation type\n",
    "from Agents.RepresentationLearning import PlaceCells\n",
    "# import actor critic network\n",
    "from Agents.Networks import ActorCritic as Network\n",
    "# import agent class wrapper to handle behaviour\n",
    "from Agents import Agent\n",
    "# import experiment class to handle run and logging\n",
    "from Experiments import expt\n",
    "\n",
    "\n",
    "# get environment\n",
    "import gym\n",
    "sys.path.append('../../../basic/')\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from Agents.EpisodicMemory import EpisodicMemory as Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 226.8x252 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make env\n",
    "env_name = 'gym_grid:gridworld-v6'\n",
    "env = gym.make(env_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_state_reps = {}\n",
    "    \n",
    "place_cells = PlaceCells(env.shape, env.nstates, field_size=0.1)\n",
    "pc_state_reps = {}\n",
    "\n",
    "for state in env.useable:\n",
    "    oh_state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "    pc_state_reps[env.twoD2oneD(state)] = place_cells.get_activities([state])[0]\n",
    "\n",
    "input_dims = len(oh_state_reps[list(oh_state_reps.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(oh_state_reps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annik/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -2.4999999999999907 (Running Avg:-2.4999999999999907) [0.26836609840393066s]\n",
      "Episode: 100, Score: 9.91 (Running Avg:1.0710000000000064) [16.03350830078125s]\n",
      "Episode: 200, Score: -2.4999999999999907 (Running Avg:1.1740000000000061) [14.655055284500122s]\n",
      "Episode: 300, Score: 9.82 (Running Avg:-0.028999999999992455) [14.93550729751587s]\n",
      "Episode: 400, Score: -2.4999999999999907 (Running Avg:2.3880000000000057) [13.82157301902771s]\n",
      "Episode: 500, Score: -2.4999999999999907 (Running Avg:-1.2619999999999916) [14.303799152374268s]\n",
      "Episode: 600, Score: -2.4999999999999907 (Running Avg:-1.2509999999999917) [14.60131287574768s]\n",
      "Episode: 700, Score: 9.91 (Running Avg:2.4650000000000056) [13.51096796989441s]\n",
      "Episode: 800, Score: 10.0 (Running Avg:-0.0029999999999924753) [14.010683298110962s]\n",
      "Episode: 900, Score: -2.4999999999999907 (Running Avg:-0.0179999999999926) [15.882170677185059s]\n",
      "Episode: 1000, Score: 9.91 (Running Avg:3.7090000000000045) [13.813767671585083s]\n",
      "Episode: 1100, Score: -2.4999999999999907 (Running Avg:-0.005999999999992856) [15.10938048362732s]\n",
      "Episode: 1200, Score: -2.4999999999999907 (Running Avg:-0.021999999999992515) [14.623498678207397s]\n",
      "Episode: 1300, Score: -2.4999999999999907 (Running Avg:-0.01399999999999264) [15.257245540618896s]\n",
      "Episode: 1400, Score: -2.4999999999999907 (Running Avg:2.457000000000006) [13.73589563369751s]\n",
      "Episode: 1500, Score: -2.4999999999999907 (Running Avg:-1.2499999999999916) [13.741854906082153s]\n",
      "Episode: 1600, Score: -2.4999999999999907 (Running Avg:1.2350000000000065) [13.228674411773682s]\n",
      "Episode: 1700, Score: 10.0 (Running Avg:2.288000000000005) [12.634911060333252s]\n",
      "Episode: 1800, Score: -2.4999999999999907 (Running Avg:4.5580000000000025) [11.183439254760742s]\n",
      "Episode: 1900, Score: -2.4999999999999907 (Running Avg:-0.050999999999992565) [12.003558158874512s]\n",
      "Episode: 2000, Score: 9.93 (Running Avg:2.449000000000005) [10.685145616531372s]\n",
      "Episode: 2100, Score: -2.4999999999999907 (Running Avg:3.5310000000000037) [9.996312618255615s]\n",
      "Episode: 2200, Score: 9.77 (Running Avg:3.6240000000000037) [11.14546823501587s]\n",
      "Episode: 2300, Score: -2.4999999999999907 (Running Avg:4.8670000000000035) [13.123706340789795s]\n",
      "Episode: 2400, Score: -2.4999999999999907 (Running Avg:-0.04199999999999262) [12.081242561340332s]\n",
      "Episode: 2500, Score: -2.4999999999999907 (Running Avg:3.6810000000000045) [11.032565355300903s]\n",
      "Episode: 2600, Score: -2.4999999999999907 (Running Avg:1.1700000000000064) [11.137912273406982s]\n",
      "Episode: 2700, Score: -2.4999999999999907 (Running Avg:2.4400000000000053) [10.878511190414429s]\n",
      "Episode: 2800, Score: -2.4999999999999907 (Running Avg:1.237000000000006) [10.899345397949219s]\n",
      "Episode: 2900, Score: -2.4999999999999907 (Running Avg:3.704000000000004) [11.306309938430786s]\n",
      "Episode: 3000, Score: -2.4999999999999907 (Running Avg:3.669000000000005) [10.738871574401855s]\n",
      "Episode: 3100, Score: -2.4999999999999907 (Running Avg:3.6820000000000035) [11.533031702041626s]\n",
      "Episode: 3200, Score: -2.4999999999999907 (Running Avg:1.221000000000006) [11.487165212631226s]\n",
      "Episode: 3300, Score: 9.74 (Running Avg:2.4300000000000055) [9.367276668548584s]\n",
      "Episode: 3400, Score: 9.83 (Running Avg:3.6910000000000047) [9.587555170059204s]\n",
      "Episode: 3500, Score: -2.4999999999999907 (Running Avg:1.2040000000000064) [13.331507444381714s]\n",
      "Episode: 3600, Score: 9.76 (Running Avg:3.6790000000000047) [10.45922040939331s]\n",
      "Episode: 3700, Score: -2.4999999999999907 (Running Avg:3.673000000000004) [10.139541149139404s]\n",
      "Episode: 3800, Score: -2.4999999999999907 (Running Avg:2.4550000000000054) [11.236688613891602s]\n",
      "Episode: 3900, Score: 9.87 (Running Avg:7.408000000000003) [9.687215328216553s]\n",
      "Episode: 4000, Score: 9.94 (Running Avg:3.732000000000005) [10.713016748428345s]\n",
      "Episode: 4100, Score: -2.4999999999999907 (Running Avg:3.690000000000004) [11.428030967712402s]\n",
      "Episode: 4200, Score: -2.4999999999999907 (Running Avg:6.184000000000003) [10.738551378250122s]\n",
      "Episode: 4300, Score: 9.94 (Running Avg:4.902000000000003) [9.774017333984375s]\n",
      "Episode: 4400, Score: -2.4999999999999907 (Running Avg:1.2260000000000066) [11.636691093444824s]\n",
      "Episode: 4500, Score: 9.96 (Running Avg:6.182000000000002) [9.883198976516724s]\n",
      "Episode: 4600, Score: -2.4999999999999907 (Running Avg:3.659000000000004) [10.934566020965576s]\n",
      "Episode: 4700, Score: 9.83 (Running Avg:3.637000000000005) [6.608752727508545s]\n",
      "Episode: 4800, Score: 9.48 (Running Avg:8.380000000000003) [4.682873725891113s]\n",
      "Episode: 4900, Score: 9.25 (Running Avg:9.538) [3.481240749359131s]\n",
      "Logged with ID d3b0c619-2774-4cf1-8045-8fc8674f909d\n"
     ]
    }
   ],
   "source": [
    "## onehot state representations\n",
    "for _ in range(1):\n",
    "    env.random_start=True\n",
    "    oh_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    oh_agent = Agent(oh_network, state_representations=oh_state_reps)\n",
    "\n",
    "    ex = expt(oh_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'onehot_train'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## place cell representations\n",
    "for _ in range(5):\n",
    "    pc_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    pc_agent = Agent(pc_network, state_representations=pc_state_reps)\n",
    "\n",
    "    ex = expt(pc_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'place_cell_train'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "    \n",
    "    ex.data['place_cells'] = place_cells\n",
    "    \n",
    "    extras = [place_cells.field_size]\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename, extra=extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place cell representations\n",
    "for f in [0.1]:\n",
    "    for e in range(5):\n",
    "        env_name = f'gym_grid:gridworld-v{e+1}'\n",
    "        env = gym.make(env_name)\n",
    "        place_cells = PlaceCells(env.shape, 200, field_size=f)\n",
    "        pc_state_reps = {}\n",
    "\n",
    "        for state in env.useable:\n",
    "            pc_state_reps[env.twoD2oneD(state)] = place_cells.get_activities([state])[0]\n",
    "            \n",
    "        input_dims = len(pc_state_reps[list(pc_state_reps.keys())[0]])\n",
    "\n",
    "        for _ in range(5):\n",
    "            pc_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "            pc_agent = Agent(pc_network, state_representations=pc_state_reps)\n",
    "\n",
    "            ex = expt(pc_agent,env)\n",
    "\n",
    "            num_trials = 5000\n",
    "            num_events = 250\n",
    "            ex.run(num_trials, num_events)\n",
    "\n",
    "            expt_type = 'place_cell_train'\n",
    "            directory = '../../Data/'\n",
    "            filename  = 'mf_training.csv'\n",
    "\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "            extras = [input_dims, place_cells.field_size]\n",
    "\n",
    "            ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename, extra=extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# successor representations\n",
    "with open(f'../../modules/Agents/RepresentationLearning/SR_{env_name}.p', 'rb') as f:\n",
    "    sr_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_reps = {}\n",
    "SR = np.sum(sr_, axis = 0)\n",
    "for index in range(SR.shape[0]):\n",
    "    sr_reps[index] = SR[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 250\n",
    "plt.figure()\n",
    "plt.imshow(sr_reps[index].reshape(*env.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## successor representations\n",
    "sr_reps = {}\n",
    "SR = np.sum(sr_, axis = 0)\n",
    "for index in range(SR.shape[0]):\n",
    "    sr_reps[index] = SR[index]\n",
    "    \n",
    "for _ in range(1):\n",
    "    input_dims = len(sr_reps[list(sr_reps.keys())[0]])\n",
    "    \n",
    "    sr_network = Network(input_dims=[input_dims],fc1_dims=200,fc2_dims=200,output_dims=env.action_space.n, lr=0.0005)\n",
    "    sr_agent = Agent(sr_network, state_representations=sr_reps)\n",
    "\n",
    "    ex = expt(sr_agent,env)\n",
    "\n",
    "    num_trials = 5000\n",
    "    num_events = 250\n",
    "    ex.run(num_trials, num_events)\n",
    "\n",
    "    expt_type = 'sr_train_with_rec'\n",
    "    directory = '../../Data/'\n",
    "    filename  = 'mf_training.csv'\n",
    "\n",
    "    ex.record_log(expt_type, env_name, num_trials, num_events, dir=directory, file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex.data['P_snap'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.gridworld_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5000-1\n",
    "plot_pref_pol(env, ex.data['P_snap'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing \n",
    "df = pd.read_csv('../../Data/mf_training.csv')\n",
    "\n",
    "for j in range(len(df)):\n",
    "    print(i)\n",
    "    i = int(j+130)\n",
    "    line = df.iloc[i]\n",
    "    if line['pass']=='pass':\n",
    "        pass\n",
    "    else:\n",
    "        run_id    = line['run_id']\n",
    "        env_type  = line['env_type']\n",
    "        expt_type = line['expt_type']\n",
    "        num_cells = line['Num_inputs']\n",
    "\n",
    "        # make environment\n",
    "        env = gym.make(env_type)\n",
    "        # remap reward \n",
    "        env.set_reward({(15,15):10})\n",
    "\n",
    "        # make network\n",
    "        network = torch.load(f'../../Data/agents/{run_id}.pt')\n",
    "\n",
    "        # get state representations\n",
    "        if expt_type == 'onehot_train':\n",
    "            print(\"Using Onehot Representation\")\n",
    "            state_reps = {}\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "\n",
    "        elif expt_type == 'place_cell_train':\n",
    "            print(\"Using Place Cell Representation\")\n",
    "            fs = line['PlaceCell_fieldsize']\n",
    "            with open(f'../../Data/results/{run_id}_data.p', 'rb') as f:\n",
    "                cell_centres = pickle.load(f)['place_cells']\n",
    "            state_reps = {}\n",
    "            pcs = PlaceCells(env.shape,num_cells=int(num_cells), field_size=fs, cell_centres=cell_centres)\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = pcs.get_activities([state])[0]\n",
    "\n",
    "        if expt_type[0:8] == 'sr_train':\n",
    "            print(\"Using Successor Representation\")\n",
    "            with open(f'../../modules/Agents/RepresentationLearning/SR_{env_type}.p', 'rb') as f:\n",
    "                sr_ = pickle.load(f)\n",
    "\n",
    "            state_reps = {}\n",
    "            SR = np.sum(sr_, axis = 0)\n",
    "            for index in range(SR.shape[0]):\n",
    "                state_reps[index] = SR[index]\n",
    "\n",
    "        # make agent\n",
    "        agent = Agent(network, state_representations=state_reps)\n",
    "\n",
    "        # make experiment\n",
    "        ex = expt(agent,env)\n",
    "\n",
    "        num_trials = 5000\n",
    "        num_events = 250\n",
    "        # run experiment\n",
    "        ex.run(num_trials, num_events)\n",
    "\n",
    "        # log results \n",
    "        expt_type = f'{expt_type[:-6]}_testing'\n",
    "        directory = '../../Data/'\n",
    "        filename  = 'mf_testing.csv'\n",
    "\n",
    "        if expt_type == 'place_cell_train':\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "        extras = [run_id]\n",
    "\n",
    "        ex.record_log(expt_type, env_type, num_trials, num_events, dir=directory, file=filename, extra=extras,\n",
    "                     mock_log=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expt_type[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_reps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expt_type[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with EC\n",
    "### testing \n",
    "df = pd.read_csv('../../Data/mf_training.csv')\n",
    "\n",
    "for j in range(len(df)):    \n",
    "    i = int(j)\n",
    "    if i ==0:\n",
    "        pass\n",
    "    line = df.iloc[i]\n",
    "    if line['pass']=='pass':\n",
    "        pass\n",
    "    else:\n",
    "        run_id    = line['run_id']\n",
    "        env_type  = line['env_type']\n",
    "        expt_type = line['expt_type']\n",
    "        num_cells = line['Num_inputs']\n",
    "\n",
    "        # make environment\n",
    "        env = gym.make(env_type)\n",
    "        # remap reward \n",
    "        env.set_reward({(15,15):10})\n",
    "\n",
    "        # make network\n",
    "        network = torch.load(f'../../Data/agents/{run_id}.pt')\n",
    "\n",
    "        # get state representations\n",
    "        if expt_type == 'onehot_train':\n",
    "            print(\"Using Onehot Representation\")\n",
    "            state_reps = {}\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = one_hot_state(env,env.twoD2oneD(state))\n",
    "\n",
    "        elif expt_type == 'place_cell_train':\n",
    "            print(\"Using Place Cell Representation\")\n",
    "            fs = line['PlaceCell_fieldsize']\n",
    "            with open(f'../../Data/results/{run_id}_data.p', 'rb') as f:\n",
    "                cell_centres = pickle.load(f)['place_cells']\n",
    "            state_reps = {}\n",
    "            pcs = PlaceCells(env.shape,num_cells=int(num_cells), field_size=fs, cell_centres=cell_centres)\n",
    "            for state in env.useable:\n",
    "                state_reps[env.twoD2oneD(state)] = pcs.get_activities([state])[0]\n",
    "\n",
    "        if expt_type[0:8] == 'sr_train':\n",
    "            print(\"Using Successor Representation\")\n",
    "            with open(f'../../modules/Agents/RepresentationLearning/SR_{env_type}.p', 'rb') as f:\n",
    "                sr_ = pickle.load(f)\n",
    "\n",
    "            state_reps = {}\n",
    "            SR = np.sum(sr_, axis = 0)\n",
    "            for index in range(SR.shape[0]):\n",
    "                state_reps[index] = SR[index]\n",
    "        \n",
    "        # make memory\n",
    "        memory = Memory(cache_limit=400, entry_size=env.action_space.n)\n",
    "        # make agent\n",
    "        agent = Agent(network, state_representations=state_reps, memory=memory)\n",
    "        agent.get_action = agent.EC_action\n",
    "        # make experiment\n",
    "        ex = expt(agent,env)\n",
    "\n",
    "        num_trials = 1000\n",
    "        num_events = 250\n",
    "        # run experiment\n",
    "        ex.run(num_trials, num_events)\n",
    "\n",
    "        # log results \n",
    "        expt_type = f'{expt_type[:-6]}_testing'\n",
    "        directory = '../../Data/'\n",
    "        filename  = 'ec_testing.csv'\n",
    "\n",
    "        if expt_type == 'place_cell_train':\n",
    "            ex.data['place_cells'] = place_cells.cell_centres\n",
    "\n",
    "        extras = [run_id]\n",
    "\n",
    "        ex.record_log(expt_type, env_type, num_trials, num_events, dir=directory, file=filename, extra=extras,\n",
    "                     mock_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
